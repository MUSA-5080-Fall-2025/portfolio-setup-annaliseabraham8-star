---
title: "Assignment 1: Census Data Quality for Policy Decisions"
subtitle: "Evaluating Data Reliability for Algorithmic Decision-Making"
author: "Annalise Abraham"
date: "September 28 2025"
format: 
  html:
    code-fold: false
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

# Assignment Overview

## Scenario

You are a data analyst for the **PA Department of Human Services**. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.

Drawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.

## Learning Objectives

- Apply dplyr functions to real census data for policy analysis
- Evaluate data quality using margins of error 
- Connect technical analysis to algorithmic decision-making
- Identify potential equity implications of data reliability issues
- Create professional documentation for policy stakeholders

## Submission Instructions

**Submit by posting your updated portfolio link on Canvas.** Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/`

Make sure to update your `_quarto.yml` navigation to include this assignment under an "Assignments" menu.

# Part 1: Portfolio Integration

Create this assignment in your portfolio repository under an `assignments/assignment_1/` folder structure. Update your navigation menu to include:

```
- text: Assignments
  menu:
    - href: assignments/assignment_1/your_file_name.qmd
      text: "Assignment 1: Census Data Exploration"
```
If there is a special character like comma, you need use double quote mark so that the quarto can identify this as text

# Setup

```{r setup}
# Load required packages (hint: you need tidycensus, tidyverse, and knitr)

library(tidycensus)
library(tidyverse)
library(knitr)
census_api_key("7f3237778c134da68ede6b2dc985410b3788e258")

# Set your Census API key

# Choose your state for analysis - assign it to a variable called my_state
```

**State Selection:** I have chosen **[Pennsylvania]** for this analysis because: [it's where I grew up and where I have lived my entire life!]

# Part 2: County-Level Resource Assessment

## 2.1 Data Retrieval

**Your Task:** Use `get_acs()` to retrieve county-level data for your chosen state.

**Requirements:**
- Geography: county level
- Variables: median household income (B19013_001) and total population (B01003_001)  
- Year: 2022
- Survey: acs5
- Output format: wide

**Hint:** Remember to give your variables descriptive names using the `variables = c(name = "code")` syntax.

```{r county-data}
# Write your get_acs() code here

pa_data <- get_acs(
  geography = "county",
  variables = c(
    total_pop = "B01003_001",
    median_income = "B19013_001"
  ),
  state = "PA",
  year = 2022,
  output = "wide"  # Makes analysis easier
)

# Clean the county names to remove state name and "County" 
# Hint: use mutate() with str_remove()

pa_data <- pa_data %>%
  mutate(
    NAME = str_remove(NAME, ", Pennsylvania"),  # remove state name
    NAME = str_remove(NAME, " County")          # remove "County"
  )

# Display the first few rows
```

## 2.2 Data Quality Assessment

**Your Task:** Calculate margin of error percentages and create reliability categories.

**Requirements:**
- Calculate MOE percentage: (margin of error / estimate) * 100
- Create reliability categories:
  - High Confidence: MOE < 5%
  - Moderate Confidence: MOE 5-10%  
  - Low Confidence: MOE > 10%
- Create a flag for unreliable estimates (MOE > 10%)

**Hint:** Use `mutate()` with `case_when()` for the categories.

pa_data_alt <- pa_data %>%
  mutate(
    moe_pct = (median_incomeM / median_incomeE) * 100,
    reliability = case_when(
      moe_pct < 5 ~ "High Confidence",
      moe_pct >= 5 & moe_pct <= 10 ~ "Moderate Confidence",
      moe_pct > 10 ~ "Low Confidence"
    ),
    unreliable_flag = moe_pct > 10
  )

```{r income-reliability}
# Calculate MOE percentage and reliability categories using mutate()

# Create a summary showing count of counties in each reliability category
# Hint: use count() and mutate() to add percentages
```

reliability_summary <- pa_data_alt %>%
  count(reliability) %>%
  mutate(
    pct = n / sum(n) * 100
  )
  
## 2.3 High Uncertainty Counties

**Your Task:** Identify the 5 counties with the highest MOE percentages.

**Requirements:**
- Sort by MOE percentage (highest first)
- Select the top 5 counties
- Display: county name, median income, margin of error, MOE percentage, reliability category
- Format as a professional table using `kable()`

**Hint:** Use `arrange()`, `slice()`, and `select()` functions.

```{r high-uncertainty}
# Create table of top 5 counties by MOE percentage

top5_moe <- pa_data_alt %>%
  arrange(desc(moe_pct)) %>%
  slice_head(n = 5) %>%
  select(
    County = NAME,
    `Median Income` = median_incomeE,
    `Margin of Error` = median_incomeM,
    `MOE %` = moe_pct,
    Reliability = reliability
  )


# Format as table with kable() - include appropriate column names and caption
```
kable(
  top5_moe,
  caption = "Top 5 Counties with Highest MOE Percentages",
  digits = 1,
  format.args = list(big.mark = ",")
)

**Data Quality Commentary:**

[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty? - Counties with lower confidence levels have less reliable data. A large range in the median income values or a small sample size could lead to higher levels of uncertainty.]

# Part 3: Neighborhood-Level Analysis

## 3.1 Focus Area Selection

**Your Task:** Select 2-3 counties from your reliability analysis for detailed tract-level study.

**Strategy:** Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.

```{r select-counties}
# Use filter() to select 2-3 counties from your county_reliability data
# Store the selected counties in a variable called selected_counties

# Display the selected counties with their key characteristics
# Show: county name, median income, MOE percentage, reliability category
```

selected_counties <- pa_data_alt %>%
  filter(NAME %in% c("Forest", 
                     "Allegheny")) %>%
  select(
    County = NAME,
    `Median Income` = median_incomeE,
    `MOE %` = moe_pct,
    Reliability = reliability
  )

selected_counties                   

**Comment on the output:** [Allegheny County has a median income of $72537 and a margin of error of 1.2% while Forest County has a median income of $46188 and a margin of error of 10%.]

## 3.2 Tract-Level Demographics

**Your Task:** Get demographic data for census tracts in your selected counties.

**Requirements:**
- Geography: tract level
- Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)
- Use the same state and year as before
- Output format: wide
- **Challenge:** You'll need county codes, not names. Look at the GEOID patterns in your county data for hints.

```{r tract-demographics}
# Define your race/ethnicity variables with descriptive names

# Use get_acs() to retrieve tract-level data
# Hint: You may need to specify county codes in the county parameter

# Calculate percentage of each group using mutate()
# Create percentages for white, Black, and Hispanic populations

# Add readable tract and county name columns using str_extract() or similar
```

library(stringr)

race_vars <- c(
  total_pop = "B03002_001",
  white_alone = "B03002_003",
  black_alone = "B03002_004",
  hispanic = "B03002_012"
)

# get tracts for Allegheny County (003) and Forest County (053) in PA
tract_demographics <- get_acs(
  geography = "tract",
  state = "42",             # Pennsylvania FIPS
  county = c("003", "053"), # county codes
  variables = race_vars,
  year = 2022,              # use same year as before
  survey = "acs5",
  output = "wide"
) %>%
  # Calculate percentages
  mutate(
    pct_white = 100 * white_aloneE / total_popE,
    pct_black = 100 * black_aloneE / total_popE,
    pct_hispanic = 100 * hispanicE / total_popE
  ) %>%
  # Extract readable county and tract codes from GEOID
  mutate(
    state_fips = str_sub(GEOID, 1, 2),
    county_fips = str_sub(GEOID, 3, 5),
    tract_code = str_sub(GEOID, 6, 11)
  )

## 3.3 Demographic Analysis

**Your Task:** Analyze the demographic patterns in your selected areas.

```{r demographic-analysis}
# Find the tract with the highest percentage of Hispanic/Latino residents
# Hint: use arrange() and slice() to get the top tract

top_hispanic_tract <- tract_demographics %>%
  arrange(desc(pct_hispanic)) %>%
  slice(1) %>%
  select(NAME, pct_white, pct_black, pct_hispanic)

# Calculate average demographics by county using group_by() and summarize()
# Show: number of tracts, average percentage for each racial/ethnic group

county_summary <- tract_demographics %>%
  group_by(county_fips) %>%   # use county code
  summarize(
    n_tracts = n(),
    avg_pct_white = mean(pct_white, na.rm = TRUE),
    avg_pct_black = mean(pct_black, na.rm = TRUE),
    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE)
  ) %>%
  ungroup()

# Create a nicely formatted table of your results using kable()
```
county_summary %>%
  kable(digits = 1, caption = "Average Demographics by County (ACS 2022 5-year)")

# Part 4: Comprehensive Data Quality Evaluation

## 4.1 MOE Analysis for Demographic Variables

**Your Task:** Examine margins of error for demographic variables to see if some communities have less reliable data.

**Requirements:**
- Calculate MOE percentages for each demographic variable
- Flag tracts where any demographic variable has MOE > 15%
- Create summary statistics

```{r demographic-moe}
# Calculate MOE percentages for white, Black, and Hispanic variables
# Hint: use the same formula as before (margin/estimate * 100)

# Create a flag for tracts with high MOE on any demographic variable
# Use logical operators (| for OR) in an ifelse() statement

# Create summary statistics showing how many tracts have data quality issues
```
demographic_moe <- tract_demographics %>%
  # 1. Calculate MOE percentages
  mutate(
    moe_white = 100 * white_aloneM / white_aloneE,
    moe_black = 100 * black_aloneM / black_aloneE,
    moe_hispanic = 100 * hispanicM / hispanicE
  ) %>%
  # 2. Replace Inf with NA
  mutate(
    across(starts_with("moe_"), ~ ifelse(is.infinite(.), NA, .))
  ) %>%
  # 3. Flag high MOE tracts (any variable > 15%)
  mutate(
    high_moe_flag = ifelse(
      moe_white > 15 | moe_black > 15 | moe_hispanic > 15,
      TRUE, FALSE
    )
  )

# 3. Summary statistics
moe_summary <- demographic_moe %>%
  summarize(
    total_tracts = n(),
    tracts_high_moe = sum(high_moe_flag, na.rm = TRUE),
    pct_high_moe = 100 * tracts_high_moe / total_tracts,
    avg_moe_white = mean(moe_white, na.rm = TRUE),
    avg_moe_black = mean(moe_black, na.rm = TRUE),
    avg_moe_hispanic = mean(moe_hispanic, na.rm = TRUE)
  )

## 4.2 Pattern Analysis

**Your Task:** Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.

```{r pattern-analysis}
# Group tracts by whether they have high MOE issues
# Calculate average characteristics for each group:
# - population size, demographic percentages

# Use group_by() and summarize() to create this comparison
# Create a professional table showing the patterns
```

pattern_analysis <- demographic_moe %>%
  group_by(high_moe_flag) %>%
  summarize(
    n_tracts = n(),
    avg_population = mean(total_popE, na.rm = TRUE),
    avg_pct_white = mean(pct_white, na.rm = TRUE),
    avg_pct_black = mean(pct_black, na.rm = TRUE),
    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE)
  ) %>%
  ungroup()

# Nicely formatted comparison table

pattern_analysis %>%
  kable(
    digits = 1,
    caption = "Comparison of Tracts by Data Quality (High MOE vs. Reliable)"
  )

**Pattern Analysis:** [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this? All the census tracts I looked at had MOE issues. The Black and Latino population data did have higher MOEs in general, perhaps because they had a smaller sample size. - Would appreciate feedback on this!]


# Part 5: Policy Recommendations

## 5.1 Analysis Integration and Professional Summary

**Your Task:** Write an executive summary that integrates findings from all four analyses.

**Executive Summary Requirements:**
1. **Overall Pattern Identification**: What are the systematic patterns across all your analyses?
2. **Equity Assessment**: Which communities face the greatest risk of algorithmic bias based on your findings?
3. **Root Cause Analysis**: What underlying factors drive both data quality issues and bias risk?
4. **Strategic Recommendations**: What should the Department implement to address these systematic issues?

**Executive Summary:**

[Our analysis of tract-level ACS data reveals that data quality problems are not randomly distributed across communities but follow clear and systematic patterns. Tracts with smaller populations consistently show higher margins of error for race and ethnicity variables, often exceeding the 15% threshold we defined for reliability concerns. In addition, tracts with higher shares of minority residents—particularly Hispanic and Black populations—are more likely to have elevated uncertainty in their demographic estimates. These findings indicate that the reliability of ACS data is unevenly distributed, with some communities consistently at a disadvantage.

This uneven distribution of reliability has important equity implications. Communities that are rural, sparsely populated, or home to significant minority populations face the greatest risk of algorithmic bias. If public agencies or partner organizations treat ACS point estimates as precise without accounting for uncertainty, decisions around resource allocation, service provision, or equity monitoring could systematically disadvantage these groups. In practice, the populations already most vulnerable to underinvestment and underrepresentation in surveys are also the ones most likely to be mischaracterized by models built on noisy or unstable data.

The root causes of these data quality disparities lie in both statistical and structural factors. Survey sampling variability is highest where population counts are low, leading to inflated margins of error in small tracts. At the same time, nonresponse and undercoverage are more common in communities with higher proportions of immigrants, renters, and low-income households. These factors compound one another, producing both unreliable estimates and heightened risk of algorithmic bias. The patterns observed are thus not only statistical artifacts but reflections of deeper structural inequities in data collection and representation.

To address these challenges, the Department should adopt a multi-layered strategy. In the short term, all products and models should explicitly incorporate margins of error, with unreliable tracts either flagged, down-weighted, or aggregated to higher levels of geography. In the medium term, statistical smoothing methods and fairness audits can reduce the impact of noisy inputs on decision-making. Over the long term, investment in targeted data collection and stronger transparency requirements will be necessary to ensure that all communities are represented with sufficient accuracy. By embedding data reliability standards into analytic workflows, the Department can both mitigate bias and build greater trust in its use of demographic data.]

## 6.3 Specific Recommendations

**Your Task:** Create a decision framework for algorithm implementation.

```{r recommendations-data}
# Create a summary table using your county reliability data
# Include: county name, median income, MOE percentage, reliability category

# Add a new column with algorithm recommendations using case_when():
# - High Confidence: "Safe for algorithmic decisions"
# - Moderate Confidence: "Use with caution - monitor outcomes"  
# - Low Confidence: "Requires manual review or additional data"

# Format as a professional table with kable()
```

county_summary <- pa_data_alt %>%
  select(
    County = NAME,
    `Median Income` = median_incomeE,
    `MOE %` = moe_pct,
    Reliability = reliability
  ) %>%
  mutate(
    Recommendation = case_when(
      Reliability == "High Confidence" ~ "Safe for algorithmic decisions",
      Reliability == "Moderate Confidence" ~ "Use with caution - monitor outcomes",
      Reliability == "Low Confidence" ~ "Requires manual review or additional data",
      TRUE ~ "Unclassified"
    )
  )

# Nicely formatted table
county_summary %>%
  kable(
    digits = 1,
    caption = "County Reliability and Algorithmic Recommendations"
  )

**Key Recommendations**

**Your Task** Use your analysis results to provide specific guidance to the department.

1. **Counties suitable for immediate algorithmic implementation:** [Counties with high confidence data are appropriate because they have low margins of error.]

2. **Counties requiring additional oversight:** [List counties with moderate confidence data and describe what kind of monitoring would be needed. The counties with moderate confidence data include Cameron, Carbon, Elk, Forest, Greene, Montour, Snyder, Sullivan, Union, and Warren counties. Monitoring could include tracking discrepancies or unusual trends or flagging certain results for human review.]

3. **Counties needing alternative approaches:** None

## Questions for Further Investigation

[List 2-3 questions that your analysis raised that you'd like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.

How can data be assessed if all tracts have high margins of error? How can we judge whether certain issues or traits are spatially signifigant?]

# Technical Notes

**Data Sources:** 
- U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates
- Retrieved via tidycensus R package on [9.29.25]

**Reproducibility:** 
- All analysis conducted in R version [your version]
- Census API key required for replication
- Complete code and documentation available at: [your portfolio URL]

**Methodology Notes:**
[Describe any decisions you made about data processing, county selection, or analytical choices that might affect reproducibility]

**Limitations:**
[Note any limitations in your analysis - sample size issues, geographic scope, temporal factors, etc.]

---

## Submission Checklist

Before submitting your portfolio link on Canvas:

- [ ] All code chunks run without errors
- [ ] All "[Fill this in]" prompts have been completed
- [ ] Tables are properly formatted and readable
- [ ] Executive summary addresses all four required components
- [ ] Portfolio navigation includes this assignment
- [ ] Census API key is properly set 
- [ ] Document renders correctly to HTML

**Remember:** Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/your_file_name.html`